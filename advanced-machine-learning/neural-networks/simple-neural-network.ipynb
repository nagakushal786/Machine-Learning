{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155b1d1d",
   "metadata": {},
   "source": [
    "## Coffee Roasting neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd54455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Normalization # type: ignore\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.losses import BinaryCrossentropy # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from lab_coffee_utils import load_coffee_data, plt_roast, plt_layer, plt_network, plt_output_unit\n",
    "\n",
    "# Allows us to manage and control the log messages\n",
    "import logging\n",
    "# Sets the logging level for tensorflow to show only errors\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "# Controls the verbosity of tensorflow autograph i.e; python code to tensorflow graph code\n",
    "tf.autograph.set_verbosity(0)\n",
    "# Disables internal logging or output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train=load_coffee_data()\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0f7b2",
   "metadata": {},
   "source": [
    "Plot the coffee roasting data below. The two features are Temperature in Celsius and Duration in minutes. `Coffee Roasting at Home` suggests that the duration is best kept between 12 and 15 minutes while the temp should be between 175 and 260 degrees Celsius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_roast(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff068d84",
   "metadata": {},
   "source": [
    "### Normalizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a228ada",
   "metadata": {},
   "source": [
    "| `axis` | Description                              | Example Shape       | Normalization Happens Across |\n",
    "| ------ | ---------------------------------------- | ------------------- | ---------------------------- |\n",
    "| `-1`   | Last axis â†’ typically **features**       | `(batch, features)` | each **column/feature**      |\n",
    "| `0`    | First axis â†’ typically **samples/batch** | `(batch, features)` | across **rows/samples**      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a14be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Temperature max, min pre normalization: {np.max(x_train[:,0]):0.2f}, {np.min(x_train[:,0]):0.2f}\")\n",
    "print(f\"Duration max, min pre normalization: {np.max(x_train[:,1]):0.2f}, {np.min(x_train[:,1]):0.2f}\")\n",
    "\n",
    "norm_l=Normalization(axis=-1)\n",
    "norm_l.adapt(x_train)\n",
    "x_norm=norm_l(x_train)\n",
    "\n",
    "print(f\"Temperature max, min post normalization: {np.max(x_norm[:,0]):0.2f}, {np.min(x_norm[:,0]):0.2f}\")\n",
    "print(f\"Duration max, min post normalization: {np.max(x_norm[:,1]):0.2f}, {np.min(x_norm[:,1]):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7deb2dd",
   "metadata": {},
   "source": [
    "### Tiling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tiled=np.tile(x_norm, (1000, 1))\n",
    "y_tiled=np.tile(y_train, (1000, 1))\n",
    "print(x_tiled.shape, y_tiled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c306ea",
   "metadata": {},
   "source": [
    "### Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ff09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "\n",
    "model=Sequential(\n",
    "  [\n",
    "    Input(shape=(2,)), # specifies the expected shape of the input\n",
    "    Dense(units=3, activation='sigmoid', name=\"L1\"),\n",
    "    Dense(units=1, activation='sigmoid', name=\"L2\")\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7fb2c7",
   "metadata": {},
   "source": [
    "### ðŸ“Š Model Summary Breakdown\n",
    "\n",
    "#### ðŸ”¹ Layer L1 (Dense with 3 units, input shape = 2)\n",
    "\n",
    "* Each of the 3 neurons receives input from **2 features**.\n",
    "* Each neuron has:\n",
    "\n",
    "  * **2 weights** (one per input)\n",
    "  * **1 bias**\n",
    "* So per neuron: `2 (weights) + 1 (bias) = 3`\n",
    "* For 3 neurons: `3 Ã— 3 = 9` parameters\n",
    "\n",
    "âœ… **L1 total parameters: 9**\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Layer L2 (Dense with 1 unit, input from 3 outputs of L1)\n",
    "\n",
    "* The single neuron in L2 receives input from **3 neurons** in L1.\n",
    "* It has:\n",
    "\n",
    "  * **3 weights**\n",
    "  * **1 bias**\n",
    "* Total: `3 + 1 = 4`\n",
    "\n",
    "âœ… **L2 total parameters: 4**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Summary Table\n",
    "\n",
    "| Layer | Inputs | Neurons | Weights per Neuron | Biases | Total Parameters |\n",
    "| ----- | ------ | ------- | ------------------ | ------ | ---------------- |\n",
    "| L1    | 2      | 3       | 2                  | 3      | 2Ã—3 + 3 = **9**  |\n",
    "| L2    | 3      | 1       | 3                  | 1      | 3Ã—1 + 1 = **4**  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1=model.get_layer(\"L1\").get_weights()\n",
    "W2, b2=model.get_layer(\"L2\").get_weights()\n",
    "\n",
    "print(W1, W1.shape, b1, b1.shape)\n",
    "print(W2, W2.shape, b2, b2.shape)\n",
    "\n",
    "# Shape of W will be (number of input features, number of units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375925a2",
   "metadata": {},
   "source": [
    "- The `model.compile` statement defines a loss function and specifies a compile optimization.\n",
    "- The `model.fit` statement runs gradient descent and fits the weights to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6680db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  loss=BinaryCrossentropy(),\n",
    "  optimizer=Adam(learning_rate=0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4739c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_tiled, y_tiled, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a560b2",
   "metadata": {},
   "source": [
    "### Epochs and batches\n",
    "In the `compile` statement above, the number of `epochs` was set to 10. This specifies that the entire data set should be applied during training 10 times.  During training, you see output describing the progress of training that looks like this:\n",
    "```\n",
    "Epoch 1/10\n",
    "6250/6250 [==============================] - 6s 910us/step - loss: 0.1782\n",
    "```\n",
    "The first line, `Epoch 1/10`, describes which epoch the model is currently running. For efficiency, the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line `6250/6250 [====` is describing which batch has been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1=model.get_layer(\"L1\").get_weights()\n",
    "W2, b2=model.get_layer(\"L2\").get_weights()\n",
    "\n",
    "print(\"Updated parameters:\")\n",
    "print(W1, b1)\n",
    "print(W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1=np.array([\n",
    "  [-8.94, 0.29, 12.89],\n",
    "  [-0.17, -7.34, 10.79]])\n",
    "b1=np.array([-9.87, -9.28,  1.01])\n",
    "\n",
    "W2=np.array([\n",
    "  [-31.38],\n",
    "  [-27.86],\n",
    "  [-32.79]])\n",
    "b2 = np.array([15.54])\n",
    "\n",
    "model.get_layer(\"L1\").set_weights([W1, b1])\n",
    "model.get_layer(\"L2\").set_weights([W2, b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2364540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.array([\n",
    "  [200, 13.9],\n",
    "  [200, 17]\n",
    "])\n",
    "\n",
    "x_test_norm=norm_l(x_test)\n",
    "predictions=model.predict(x_test_norm)\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc61be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.zeros_like(predictions)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "  if predictions[i]>=0.5:\n",
    "    y_pred[i]=1\n",
    "  else:\n",
    "    y_pred[i]=0\n",
    "\n",
    "print(f\"Decisions: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_layer(x_train, y_train.reshape(-1,), W1, b1, norm_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540fa712",
   "metadata": {},
   "source": [
    "The shading shows that each unit is responsible for a different `bad roast` region. Unit 0 has larger values when the temperature is too low. Unit 1 has larger values when the duration is too short. Unit 2 has larger values for bad combinations of duration and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc406018",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_output_unit(W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a45e7",
   "metadata": {},
   "source": [
    "High output values correspond to `bad roast` areas. Below, the maximum output is in areas where the three inputs are small values corresponding to `good roast` areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "netf=lambda x: model.predict(norm_l(x))\n",
    "plt_network(x_train, y_train, netf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c669347a",
   "metadata": {},
   "source": [
    "The left graph is the raw output of the final layer represented by the blue shading. This is overlaid on the training data represented by the X's and O's.   \n",
    "\n",
    "The right graph is the output of the network after a decision threshold. The X's and O's here correspond to decisions made by the network.  \n",
    "The following takes a moment to run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

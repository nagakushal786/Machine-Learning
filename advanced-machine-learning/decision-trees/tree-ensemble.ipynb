{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec3fbbf",
   "metadata": {},
   "source": [
    "## Tree Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de778e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_STATE=55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b77b3e",
   "metadata": {},
   "source": [
    "### Dataset Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ffc8d",
   "metadata": {},
   "source": [
    "- `Age`: Age of the patient [years]\n",
    "\n",
    "- `Sex`: Sex of the patient [M: Male, F: Female]\n",
    "\n",
    "- `ChestPainType`: Chest Pain Type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
    "\n",
    "- `RestingBP`: Resting Blood Pressure [mm Hg]\n",
    "\n",
    "- `Cholesterol`: Serum Cholesterol [mm/dl]\n",
    "\n",
    "- `FastingBS`: Fasting Blood Sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
    "\n",
    "- `RestingECG`: Resting Electro Cardiogram Results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
    "\n",
    "- `MaxHR`: Maximum Heart Rate achieved [Numeric value between 60 and 202]\n",
    "\n",
    "- `ExerciseAngina`: Exercise-Induced Angina [Y: Yes, N: No]\n",
    "\n",
    "- `Oldpeak`: Oldpeak = ST [Numeric value measured in depression]\n",
    "\n",
    "- `ST_Slope`: the Slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
    "\n",
    "- `HeartDisease`: Output class [1: Heart disease, 0: Normal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358212c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df=pd.read_csv(\"heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ac059",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38727f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already existing binary features\n",
    "cat_bin_features=['Sex', 'ChestPainType', 'RestingECG',\n",
    "                  'ExerciseAngina', 'ST_Slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f632c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will replace the columns with the one-hot encoded ones and keep the columns outside 'columns' argument as it is\n",
    "heart_ohe_df=pd.get_dummies(data=heart_df, prefix=cat_bin_features, columns=cat_bin_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_ohe_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42970611",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_ohe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31737c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing our target variable\n",
    "features=[x for x in heart_ohe_df.columns if x not in \"HeartDisease\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f6fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1790219",
   "metadata": {},
   "source": [
    "### Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c831acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits arrays or matrices into random training and test subsets\n",
    "x_train, x_val, y_train, y_val=train_test_split(\n",
    "  heart_ohe_df[features],\n",
    "  heart_ohe_df[\"HeartDisease\"],\n",
    "  train_size=0.8,\n",
    "  random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training samples: {len(x_train)}\")\n",
    "print(f\"Number of validation samples: {len(x_val)}\")\n",
    "print(f\"Target proportion: {sum(y_train)/len(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe7908",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98019971",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split_list=[2, 10, 30, 50, 100, 200, 300, 700]\n",
    "max_depth_list=[1, 2, 3, 4, 8, 16, 32, 64, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b11db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_train=[]\n",
    "accuracy_list_validation=[]\n",
    "\n",
    "for min_samples_split in min_samples_split_list:\n",
    "  model=DecisionTreeClassifier(min_samples_split=min_samples_split,\n",
    "                               random_state=RANDOM_STATE).fit(x_train, y_train)\n",
    "  \n",
    "  predictions_train=model.predict(x_train)\n",
    "  predictions_val=model.predict(x_val)\n",
    "  accuracy_train=accuracy_score(predictions_train, y_train)\n",
    "  accuracy_val=accuracy_score(predictions_val, y_val)\n",
    "\n",
    "  accuracy_list_train.append(accuracy_train)\n",
    "  accuracy_list_validation.append(accuracy_val)\n",
    "\n",
    "plt.title(\"Training x Validation Metrics\")\n",
    "plt.xlabel(\"Minimum Samples Split\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(range(len(min_samples_split_list)), labels=min_samples_split_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_validation)\n",
    "plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e24e1c",
   "metadata": {},
   "source": [
    "Increasing the the number of `min_samples_split` reduces overfitting.\n",
    "\n",
    "- Increasing `min_samples_split` from 10 to 30, and from 30 to 50, even though it does not improve the validation accuracy, it brings the training accuracy closer to it, showing a reduction in overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb6a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_train=[]\n",
    "accuracy_list_validation=[]\n",
    "\n",
    "for max_depth in max_depth_list:\n",
    "  model=DecisionTreeClassifier(max_depth=max_depth,\n",
    "                               random_state=RANDOM_STATE).fit(x_train, y_train)\n",
    "  \n",
    "  predictions_train=model.predict(x_train)\n",
    "  predictions_val=model.predict(x_val)\n",
    "  accuracy_train=accuracy_score(predictions_train, y_train)\n",
    "  accuracy_val=accuracy_score(predictions_val, y_val)\n",
    "\n",
    "  accuracy_list_train.append(accuracy_train)\n",
    "  accuracy_list_validation.append(accuracy_val)\n",
    "\n",
    "plt.title(\"Training x Validation Metrics\")\n",
    "plt.xlabel(\"Maximum Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(range(len(max_depth_list)), labels=max_depth_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_validation)\n",
    "plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024496d8",
   "metadata": {},
   "source": [
    "We can see that in general, reducing `max_depth` can help to reduce overfitting.\n",
    "\n",
    "- Reducing `max_depth` from 8 to 4 increases validation accuracy closer to training accuracy, while significantly reducing training accuracy.\n",
    "\n",
    "- The validation accuracy reaches the highest at tree_depth=4. \n",
    "\n",
    "- When the `max_depth` is smaller than 3, both training and validation accuracy decreases.  The tree cannot make enough splits to distinguish positives from negatives (the model is underfitting the training set).\n",
    " \n",
    "- When the `max_depth` is too high ( >= 5), validation accuracy decreases while training accuracy increases, indicating that the model is overfitting to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model=DecisionTreeClassifier(min_samples_split=50,\n",
    "                                           max_depth=4,\n",
    "                                           random_state=RANDOM_STATE).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Metrics:\\n\\tAccuracy Score: {accuracy_score(decision_tree_model.predict(x_train), y_train)}\")\n",
    "print(f\"Validation Metrics:\\n\\tAccuracy Score: {accuracy_score(decision_tree_model.predict(x_val), y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628f19d",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8062f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_list=[2, 4, 8, 16, 32, 64, None]\n",
    "n_estimators_list=[10, 50, 100, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_train=[]\n",
    "accuracy_list_validation=[]\n",
    "\n",
    "for min_samples_split in min_samples_split_list:\n",
    "  model=RandomForestClassifier(min_samples_split=min_samples_split,\n",
    "                               random_state=RANDOM_STATE).fit(x_train, y_train)\n",
    "  \n",
    "  predictions_train=model.predict(x_train)\n",
    "  predictions_val=model.predict(x_val)\n",
    "  accuracy_train=accuracy_score(predictions_train, y_train)\n",
    "  accuracy_val=accuracy_score(predictions_val, y_val)\n",
    "\n",
    "  accuracy_list_train.append(accuracy_train)\n",
    "  accuracy_list_validation.append(accuracy_val)\n",
    "\n",
    "plt.title(\"Training x Validation Metrics\")\n",
    "plt.xlabel(\"Minimum Samples Split\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(range(len(min_samples_split_list)), labels=min_samples_split_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_validation)\n",
    "plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_train=[]\n",
    "accuracy_list_validation=[]\n",
    "\n",
    "for max_depth in max_depth_list:\n",
    "  model=RandomForestClassifier(max_depth=max_depth,\n",
    "                               random_state=RANDOM_STATE).fit(x_train, y_train)\n",
    "  \n",
    "  predictions_train=model.predict(x_train)\n",
    "  predictions_val=model.predict(x_val)\n",
    "  accuracy_train=accuracy_score(predictions_train, y_train)\n",
    "  accuracy_val=accuracy_score(predictions_val, y_val)\n",
    "\n",
    "  accuracy_list_train.append(accuracy_train)\n",
    "  accuracy_list_validation.append(accuracy_val)\n",
    "\n",
    "plt.title(\"Training x Validation Metrics\")\n",
    "plt.xlabel(\"Maximum Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(range(len(max_depth_list)), labels=max_depth_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_validation)\n",
    "plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_train=[]\n",
    "accuracy_list_validation=[]\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "  model=RandomForestClassifier(n_estimators=n_estimators,\n",
    "                               random_state=RANDOM_STATE).fit(x_train, y_train)\n",
    "  \n",
    "  predictions_train=model.predict(x_train)\n",
    "  predictions_val=model.predict(x_val)\n",
    "  accuracy_train=accuracy_score(predictions_train, y_train)\n",
    "  accuracy_val=accuracy_score(predictions_val, y_val)\n",
    "\n",
    "  accuracy_list_train.append(accuracy_train)\n",
    "  accuracy_list_validation.append(accuracy_val)\n",
    "\n",
    "plt.title(\"Training x Validation Metrics\")\n",
    "plt.xlabel(\"Number of Estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(range(len(n_estimators_list)), labels=n_estimators_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_validation)\n",
    "plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a3bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forst_model=RandomForestClassifier(n_estimators=100,\n",
    "                                          max_depth=16,\n",
    "                                          min_samples_split=10).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Metrics:\\n\\tAccuracy Score: {accuracy_score(random_forst_model.predict(x_train), y_train)}\")\n",
    "print(f\"Validation Metrics:\\n\\tAccuracy Score: {accuracy_score(random_forst_model.predict(x_val), y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fac6ca",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf1d8e5",
   "metadata": {},
   "source": [
    "The boosting methods train several trees, but instead of them being uncorrelated to each other, now the trees are fit one after the other in order to minimize the error.\n",
    "\n",
    "- The learning rate is the size of the step on the Gradient Descent method that the XGBoost uses internally to minimize the error on each train step.\n",
    "\n",
    "XGBoost can take in an evaluation dataset of the form `(X_val,y_val)`.\n",
    "- On each iteration, it measures the cost (or evaluation metric) on the evaluation datasets.\n",
    "\n",
    "- Once the cost (or metric) stops decreasing for a number of rounds (called early_stopping_rounds), the training will stop.\n",
    "\n",
    "- More iterations lead to more estimators, and more estimators can result in overfitting.\n",
    "\n",
    "- By stopping once the validation metric no longer improves, we can limit the number of estimators created, and reduce overfitting.\n",
    "\n",
    "- `eval_set = [(X_train_eval,y_train_eval)]`:Here we must pass a list to the eval_set, because you can have several different tuples ov eval sets.\n",
    "\n",
    "- `early_stopping_rounds`: This parameter helps to stop the model training if its evaluation metric is no longer improving on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0eeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% to train and 20% to test\n",
    "n=int(len(x_train)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d459f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_fit, x_train_eval, y_train_fit, y_train_eval=x_train[:n], x_train[n:], y_train[:n], y_train[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6eff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model=XGBClassifier(n_estimators=500, learning_rate=0.1, \n",
    "                        verbosity=1, random_state=RANDOM_STATE)\n",
    "\n",
    "xgb_model.fit(x_train_fit, y_train_fit,\n",
    "              eval_set=[(x_train_eval, y_train_eval)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Metrics:\\n\\tAccuracy Score: {accuracy_score(xgb_model.predict(x_train), y_train)}\")\n",
    "print(f\"Validation Metrics:\\n\\tAccuracy Score: {accuracy_score(xgb_model.predict(x_val), y_val)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

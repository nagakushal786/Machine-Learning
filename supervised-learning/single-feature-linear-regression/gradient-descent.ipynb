{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f2b840",
   "metadata": {},
   "source": [
    "## Gradient descent for single feature linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7267b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5de819",
   "metadata": {},
   "source": [
    "A simple data set with only two data points - a house with 1000 square feet(sqft) sold for `$300,000` and a house with 2000 square feet sold for `$500,000`. These two points will constitute our *data or training set*. Here, the units of size are 1000 sqft and the units of price are 1000s of dollars.\n",
    "\n",
    "| Size (1000 sqft)     | Price (1000s of dollars) |\n",
    "| -------------------| ------------------------ |\n",
    "| 1.0               | 300                      |\n",
    "| 2.0               | 500                      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array([1.0, 2.0])\n",
    "y_train=np.array([300.0, 500.0])\n",
    "\n",
    "print(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(x, y, w, b):\n",
    "  m=x.shape[0]\n",
    "\n",
    "  cost_sum=0\n",
    "  for i in range(m):\n",
    "    f_wb=w*x[i]+b\n",
    "    cost=(f_wb-y[i])**2\n",
    "    cost_sum=cost_sum+cost\n",
    "\n",
    "  total_cost=(1/(2*m))*cost_sum\n",
    "\n",
    "  return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b):\n",
    "  m=x.shape[0]\n",
    "  dj_dw=0\n",
    "  dj_db=0\n",
    "\n",
    "  for i in range(m):\n",
    "    f_wb=w*x[i]+b\n",
    "\n",
    "    dj_dw_i=(f_wb-y[i])*x[i]\n",
    "    dj_db_i=f_wb-y[i]\n",
    "\n",
    "    dj_dw+=dj_dw_i\n",
    "    dj_db+=dj_db_i\n",
    "\n",
    "  dj_dw=dj_dw/m\n",
    "  dj_db=dj_db/m\n",
    "\n",
    "  return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb38c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_gradients(x_train, y_train, compute_cost_function, compute_gradient)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98780e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "  w=copy.deepcopy(w_in)\n",
    "\n",
    "  J_history=[]\n",
    "  p_history=[]\n",
    "  b=b_in\n",
    "  w=w_in\n",
    "\n",
    "  for i in range(num_iters):\n",
    "    dj_dw, dj_db=compute_gradient(x, y, w, b)\n",
    "\n",
    "    b=b-alpha*dj_db\n",
    "    w=w-alpha*dj_dw\n",
    "\n",
    "    if i<100000:\n",
    "      J_history.append(cost_function(x, y, w, b))\n",
    "      p_history.append([w, b])\n",
    "\n",
    "    # Print every cost at intervals 10 times or as many if <10\n",
    "    if i%math.ceil(num_iters/10)==0:\n",
    "      print(f'Iteration {i:4}: Cost {J_history[-1]:0.2e} ',\n",
    "            f'dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}',\n",
    "            f'w: {w: 0.3e}, b: {b: 0.3e}')\n",
    "      \n",
    "  return w, b, J_history, p_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init=0\n",
    "b_init=0\n",
    "\n",
    "iterations=10000\n",
    "tmp_alpha=1.0e-2\n",
    "\n",
    "w_final, b_final, J_hist, p_hist=gradient_descent(x_train, y_train, w_init, b_init, tmp_alpha, iterations, compute_cost_function, compute_gradient)\n",
    "\n",
    "print(f'(w, b) found by gradient descent: ({w_final: 8.4f}, {b_final: 8.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a38e36",
   "metadata": {},
   "source": [
    "### Cost versus iterations of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d89ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2)=plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000+np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "\n",
    "ax1.set_title('Cost vs iterations(start)')\n",
    "ax2.set_title('Cost vs iterations(end)')\n",
    "\n",
    "ax1.set_ylabel('Cost')\n",
    "ax1.set_xlabel('Iteration step')\n",
    "\n",
    "ax2.set_ylabel('Cost')\n",
    "ax2.set_xlabel('Iteration step')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cff6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(1, 1, figsize=(12, 6))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f6d0e",
   "metadata": {},
   "source": [
    "Above, the contour plot shows the $cost(w,b)$ over a range of $w$ and $b$. Cost levels are represented by the rings. Overlayed, using red arrows, is the path of gradient descent. Here are some things to note:\n",
    "- The path makes steady (monotonic) progress toward its goal.\n",
    "- initial steps are much larger than the steps near the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To observe the final steps of gradient descent\n",
    "\n",
    "fig, ax=plt.subplots(1, 1, figsize=(12, 4))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[180, 220, 0.5],\n",
    "                  b_range=[80, 120, 0.5], contours=[1, 5, 10, 20], resolution=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534fee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init=0\n",
    "b_init=0\n",
    "\n",
    "iterations=10\n",
    "tmp_alpha=8.0e-1\n",
    "\n",
    "w_final, b_final, J_hist, p_hist=gradient_descent(x_train, y_train, w_init, b_init, tmp_alpha, iterations, compute_cost_function, compute_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3825b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_divergence(p_hist, J_hist, x_train, y_train)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

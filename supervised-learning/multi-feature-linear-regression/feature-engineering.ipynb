{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe50226c",
   "metadata": {},
   "source": [
    "## Feature engineering and polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e80bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_multi import zscore_normalize_features, run_gradient_descent_feng\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba53bf",
   "metadata": {},
   "source": [
    "### Polynomial features\n",
    "\n",
    "We are considering a scenario where the data was non-linear. Let's try using what we know so far to fit a non-linear curve. We'll start with a simple quadratic: $y = 1+x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0, 20, 1)\n",
    "y=1+x**2\n",
    "X=x.reshape(-1, 1)\n",
    "\n",
    "model_w, model_b=run_gradient_descent_feng(X, y, iterations=1000, alpha=1e-2)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label='Actual value')\n",
    "plt.title('No feature engineering')\n",
    "plt.plot(x, X@model_w+model_b, label='Predicted value')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6a4ac",
   "metadata": {},
   "source": [
    "As expected, not a great fit. What is needed is something like $y= w_0x_0^2 + b$, or a **polynomial feature**.\n",
    "To accomplish this, you can modify the *input data* to *engineer* the needed features. If you swap the original data with a version that squares the $x$ value, then you can achieve $y= w_0x_0^2 + b$. Let's try it. Swap `X` for `X**2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e92bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0, 20, 1)\n",
    "y=1+x**2\n",
    "\n",
    "X=x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape(-1, 1)\n",
    "\n",
    "model_w, model_b=run_gradient_descent_feng(X, y, iterations=10000, alpha=1e-5)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label='Actual value')\n",
    "plt.title('Added x**2 feature')\n",
    "plt.plot(x, X@model_w+model_b, label='Predicted value')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7ec9d",
   "metadata": {},
   "source": [
    "Notice the values of $\\mathbf{w}$ and b printed right above the graph: `w,b found by gradient descent: w: [1.], b: 0.0490`. Gradient descent modified our initial values of $\\mathbf{w},b $ to be (1.0,0.049) or a model of $y=1*x_0^2+0.049$, very close to our target of $y=1*x_0^2+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea21aaf",
   "metadata": {},
   "source": [
    "### Selecting features\n",
    "\n",
    "Above, we knew that an $x^2$ term was required. It may not always be obvious which features are required. One could add a variety of potential features to try and find the most useful. For example, what if we had instead tried : $y=w_0x_0 + w_1x_1^2 + w_2x_2^3+b$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0, 20, 1)\n",
    "y=x**2\n",
    "\n",
    "X=np.c_[x, x**2, x**3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaacbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w, model_b=run_gradient_descent_feng(X, y, iterations=10000, alpha=1e-7)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label='Actual value')\n",
    "plt.title('x, x**2, x**3 features')\n",
    "plt.plot(x, X@model_w+model_b, label='Predicted value')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677092c8",
   "metadata": {},
   "source": [
    "Let's review this idea:\n",
    "- Intially, the features were re-scaled so they are comparable to each other\n",
    "- less weight value implies less important/correct feature, and in extreme, when the weight becomes zero or very close to zero, the associated feature useful in fitting the model to the data.\n",
    "- above, after fitting, the weight associated with the $x^2$ feature is much larger than the weights for $x$ or $x^3$ as it is the most useful in fitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d7d1fe",
   "metadata": {},
   "source": [
    "### Alternate view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0, 20, 1)\n",
    "y=x**2\n",
    "\n",
    "X=np.c_[x, x**2, x**3]\n",
    "X_features=['x', 'x**2', 'x**3']\n",
    "\n",
    "fig, ax=plt.subplots(1, 3, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "  ax[i].scatter(X[:, i], y)\n",
    "  ax[i].set_xlabel(X_features[i])\n",
    "\n",
    "ax[0].set_ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79586f4e",
   "metadata": {},
   "source": [
    "### Scaling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0, 20, 1)\n",
    "X=np.c_[x, x**2, x**3]\n",
    "\n",
    "print(f'Peak to peak range by column in Raw form        X: {np.ptp(X, axis=0)}')\n",
    "\n",
    "X=zscore_normalize_features(X)\n",
    "print(f'Peak to peak range by column in Normalized form X: {np.ptp(X, axis=0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0, 20, 1)\n",
    "y=x**2\n",
    "\n",
    "X=np.c_[x, x**2, x**3]\n",
    "X=zscore_normalize_features(X)\n",
    "\n",
    "model_w, model_b=run_gradient_descent_feng(X, y, iterations=100000, alpha=1e-1)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label='Actual value')\n",
    "plt.title('Normalized x, x**2, x**3 features')\n",
    "plt.plot(x, X@model_w+model_b, label='Predicted value')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd3a73",
   "metadata": {},
   "source": [
    "Feature scaling allows this to converge much faster.   \n",
    "Note again the values of $\\mathbf{w}$. The $w_1$ term, which is the $x^2$ term is the most emphasized. Gradient descent has all but eliminated the $x^3$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d8648",
   "metadata": {},
   "source": [
    "### Complex functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f184a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0, 20, 1)\n",
    "y=np.cos(x/2)\n",
    "\n",
    "X=np.c_[x, x**2, x**3, x**4, x**5, x**6, x**7, x**8, x**9, x**10, x**11, x**12, x**13]\n",
    "X=zscore_normalize_features(X)\n",
    "\n",
    "model_w, model_b=run_gradient_descent_feng(X, y, iterations=100000, alpha=1e-1)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label='Actual value')\n",
    "plt.title('Normalized x, x**2, x**3 features')\n",
    "plt.plot(x, X@model_w+model_b, label='Predicted value')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
